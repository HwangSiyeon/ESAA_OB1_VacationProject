{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 아이템 기반\n","0.3143420016   \n","0.2953427896"],"metadata":{"id":"mVrT_kar4uZK"}},{"cell_type":"code","source":["article_info = pd.read_csv('/content/article_info.csv')\n","view_log = pd.read_csv('/content/view_log.csv')\n","sample_submission = pd.read_csv('/content/sample_submission.csv')"],"metadata":{"id":"6MEQjHnm6728"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["article_info.rename(columns={'userID':'reporterID'}, inplace=True)\n","view_log.rename(columns={'userID':'readerID'}, inplace = True)"],"metadata":{"id":"6Eag_25zIzpw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 사용자-기사 행렬 생성\n","user_article_matrix = view_log.groupby(['articleID', 'readerID']).size().unstack(fill_value=0)"],"metadata":{"id":"Rt4SaaRZITkU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#코사인유사도\n","from sklearn.metrics.pairwise import cosine_similarity\n","item_based_collabor = cosine_similarity(user_article_matrix)\n","item_based_collabor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14egroVuJDzz","outputId":"6134ddad-2243-43c8-d00c-329946eaf408"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.        , 0.04811252, 0.        , ..., 0.02868877, 0.06154575,\n","        0.        ],\n","       [0.04811252, 1.        , 0.        , ..., 0.        , 0.05330018,\n","        0.15075567],\n","       [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n","        0.        ],\n","       ...,\n","       [0.02868877, 0.        , 0.        , ..., 1.        , 0.        ,\n","        0.        ],\n","       [0.06154575, 0.05330018, 0.        , ..., 0.        , 1.        ,\n","        0.        ],\n","       [0.        , 0.15075567, 0.        , ..., 0.        , 0.        ,\n","        1.        ]])"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# 기사 ID -> 인덱스 매핑\n","article_id_to_idx = {article: idx for idx, article in enumerate(user_article_matrix.index)}\n","\n","def recommend_articles(user_id, view_log, article_similarity, article_id_to_idx, top_n=5):\n","    user_articles = view_log[view_log['readerID'] == user_id]['articleID'].tolist()\n","\n","    # 조회한 기사와 유사한 기사 추출\n","    sim_scores = pd.Series(dtype=float)\n","    for article in user_articles:\n","        if article in article_id_to_idx:\n","            article_idx = article_id_to_idx[article]\n","            if article_idx < article_similarity.shape[0]:  # 인덱스 범위 확인\n","                sim_scores = sim_scores.add(pd.Series(article_similarity[article_idx]), fill_value=0)\n","\n","    # 조회한 기사는 제외\n","    #sim_scores = sim_scores.drop([article_id_to_idx[article] for article in user_articles if article in article_id_to_idx and article_id_to_idx[article] < article_similarity.shape[0]], errors='ignore')\n","\n","    # 유사도 높은 기사 추천\n","    top_articles = sim_scores.sort_values(ascending=False).head(top_n).index\n","    top_article_ids = [user_article_matrix.index[idx] for idx in top_articles]\n","\n","    return top_article_ids\n","\n","# 각 사용자에 대해 추천 기사 생성\n","recommendations = []\n","for user_id in sample_submission['userID'].unique():\n","    recommended_articles = recommend_articles(user_id, view_log, item_based_collabor, article_id_to_idx, top_n=5)\n","    for article_id in recommended_articles:\n","        recommendations.append({'userID': user_id, 'articleID': article_id})\n","\n","# 추천 결과 저장\n","submission_df = pd.DataFrame(recommendations)\n","submission_df.to_csv('recommendations.csv', index=False)"],"metadata":{"id":"gF9-JZ-6NhbS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 잠재요인-튜닝\n","0.0448778566  \n","0.0448345154"],"metadata":{"id":"-7nMJmY5Qhmy"}},{"cell_type":"code","source":["article_info.rename(columns={'userID':'reporterID'}, inplace=True)\n","view_log.rename(columns={'userID':'readerID'}, inplace = True)"],"metadata":{"id":"qNHDufobRtlF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"apiff18bQSGJ","outputId":"24074481-1c7f-405e-fa67-9728735fd8f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["K: 10, steps: 100, learning_rate: 0.01, r_lambda: 0.01, recall: 0.0021367762899136226\n","K: 10, steps: 100, learning_rate: 0.001, r_lambda: 0.01, recall: 0.007332597247022509\n","K: 10, steps: 200, learning_rate: 0.01, r_lambda: 0.01, recall: 0.0023984729343515723\n","K: 10, steps: 200, learning_rate: 0.001, r_lambda: 0.01, recall: 0.0046428485731918505\n","K: 10, steps: 300, learning_rate: 0.01, r_lambda: 0.01, recall: 0.0023895112532487865\n","K: 10, steps: 300, learning_rate: 0.001, r_lambda: 0.01, recall: 0.004208366675003056\n","K: 30, steps: 100, learning_rate: 0.01, r_lambda: 0.01, recall: 0.0031694397150048097\n","K: 30, steps: 100, learning_rate: 0.001, r_lambda: 0.01, recall: 0.01741466532838933\n","K: 30, steps: 200, learning_rate: 0.01, r_lambda: 0.01, recall: 0.003136554862598045\n","K: 30, steps: 200, learning_rate: 0.001, r_lambda: 0.01, recall: 0.006171342465672139\n","K: 30, steps: 300, learning_rate: 0.01, r_lambda: 0.01, recall: 0.0031525438621663416\n","K: 30, steps: 300, learning_rate: 0.001, r_lambda: 0.01, recall: 0.005213852002279673\n","Best parameters: {'K': 30, 'steps': 100, 'learning_rate': 0.001, 'r_lambda': 0.01}\n","Best recall: 0.01741466532838933\n"]}],"source":["from sklearn.metrics import mean_squared_error\n","# 사용자-기사 행렬 생성\n","user_article_matrix = view_log.groupby(['readerID', 'articleID']).size().unstack(fill_value=0)\n","\n","# 훈련 데이터와 검증 데이터로 나누기\n","train_data, val_data = train_test_split(user_article_matrix, test_size=0.2, random_state=42)\n","\n","def get_rmse(R, P, Q, non_zeros):\n","    full_pred_matrix = np.dot(P, Q.T)\n","    x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]\n","    y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]\n","    R_non_zeros = R[x_non_zero_ind, y_non_zero_ind]\n","    full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]\n","    mse = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)\n","    rmse = np.sqrt(mse)\n","    return rmse\n","\n","def matrix_factorization(R, K, steps=200, learning_rate=0.01, r_lambda=0.01):\n","    num_users, num_items = R.shape\n","    np.random.seed(1)\n","    P = np.random.normal(scale=1./K, size=(num_users, K))\n","    Q = np.random.normal(scale=1./K, size=(num_items, K))\n","    non_zeros = [(i, j, R[i, j]) for i in range(num_users) for j in range(num_items) if R[i, j] > 0]\n","\n","    for step in range(steps):\n","        for i, j, r in non_zeros:\n","            eij = r - np.dot(P[i, :], Q[j, :].T)\n","            P[i, :] += learning_rate * (eij * Q[j, :] - r_lambda * P[i, :])\n","            Q[j, :] += learning_rate * (eij * P[i, :] - r_lambda * Q[j, :])\n","\n","    return P, Q\n","\n","def calculate_recall_at_k(val_data, pred_matrix, k=5):\n","    num_users = val_data.shape[0]\n","    recalls = []\n","\n","    for user in range(num_users):\n","        actual_items = val_data.columns[val_data.iloc[user, :].to_numpy().nonzero()]\n","        predicted_items = np.argsort(pred_matrix[user, :])[::-1][:k]\n","        predicted_items = val_data.columns[predicted_items]\n","        hit_count = len(set(actual_items) & set(predicted_items))\n","        recall = hit_count / len(actual_items) if len(actual_items) > 0 else 0\n","        recalls.append(recall)\n","\n","    return np.mean(recalls)\n","\n","# 하이퍼파라미터 튜닝을 위한 파라미터 범위 설정\n","K_values = [10, 30]\n","steps_values = [100, 200, 300]\n","learning_rate_values = [0.01, 0.001]\n","r_lambda_values = [0.01]\n","\n","best_params = {}\n","best_recall = -1\n","\n","for K in K_values:\n","    for steps in steps_values:\n","        for learning_rate in learning_rate_values:\n","            for r_lambda in r_lambda_values:\n","                P, Q = matrix_factorization(train_data.values, K, steps, learning_rate, r_lambda)\n","                pred_matrix = np.dot(P, Q.T)\n","                recall = calculate_recall_at_k(val_data, pred_matrix, k=5)\n","                print(f'K: {K}, steps: {steps}, learning_rate: {learning_rate}, r_lambda: {r_lambda}, recall: {recall}')\n","\n","                if recall > best_recall:\n","                    best_recall = recall\n","                    best_params = {'K': K, 'steps': steps, 'learning_rate': learning_rate, 'r_lambda': r_lambda}\n","\n","print(\"Best parameters:\", best_params)\n","print(\"Best recall:\", best_recall)\n","\n","# 최적의 하이퍼파라미터로 전체 데이터에 대해 모델 학습\n","P, Q = matrix_factorization(user_article_matrix.values, best_params['K'], best_params['steps'], best_params['learning_rate'], best_params['r_lambda'])\n","pred_matrix = np.dot(P, Q.T)\n","pred_matrix = pd.DataFrame(data=pred_matrix, index=user_article_matrix.index, columns=user_article_matrix.columns)\n","\n","# 사용자에게 추천할 기사를 추출하는 함수\n","def recommend_articles(reader_id, matrix, top_n=5):\n","    scores = matrix.loc[reader_id]\n","    top_articles = scores.sort_values(ascending=False).head(top_n).index\n","    return top_articles.tolist()\n","\n","# 각 사용자에 대해 추천 기사 생성\n","recommendations = []\n","for reader_id in sample_submission['userID'].unique():\n","    recommended_articles = recommend_articles(reader_id, pred_matrix, top_n=5)\n","    for article_id in recommended_articles:\n","        recommendations.append({'userID': reader_id, 'articleID': article_id})\n","\n","# 추천 결과 저장\n","submission_df = pd.DataFrame(recommendations)\n","submission_df.to_csv('잠재요인_행렬분해_튜닝.csv', index=False)"]},{"cell_type":"markdown","source":["# 컨텐츠 기반  \n","- 번역 후 클렌징한 뒤 tfidf   \n","\t0.3273443656  \n","  0.3171040189"],"metadata":{"id":"6FCwW8CbmH02"}},{"cell_type":"code","source":["# 구글 번역기 설치 (처음에만 실행)\n","!pip install googletrans==4.0.0-rc1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ldkoQtQmKxR","outputId":"c2a20a42-c3b9-4699-e948-961e73c1df13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting googletrans==4.0.0-rc1\n","  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n","  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.6.2)\n","Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hstspreload-2024.7.1-py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=f306e9b5d3b74b1c1b015b9351061fc59911269ebf4fc2d9758ce29fc70340fb\n","  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.7\n","    Uninstalling idna-3.7:\n","      Successfully uninstalled idna-3.7\n","Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.7.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from googletrans import Translator\n","\n","translator = Translator()\n","\n","def translate_text(text, translator):\n","    try:\n","        return translator.translate(text, dest='en', src='auto').text\n","    except Exception as e:\n","        return text  # translation failed, return the original text\n","\n","# 대상 언어를 리스트로 정의\n","target_languages = ['pt', 'la', 'es', 'ja']\n","\n","# 'Language' 열이 target_languages에 포함된 행에 대해 번역 수행\n","mask = article_info['Language'].isin(target_languages)\n","article_info.loc[mask, 'Title'] = article_info.loc[mask, 'Title'].apply(translate_text, translator=translator)"],"metadata":{"id":"VzYW_O_ZoHjc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","import re\n","import string\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSkhaoybbQc9","outputId":"18066402-eb23-4ee7-9c64-20ed44a7aa28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["## Title 텍스트 클렌징해보자\n","contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n","                        \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n","                        \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n","                        \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\",\n","                        \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\"}\n","def _get_contractions(contraction_dict):\n","        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n","        return contraction_dict, contraction_re\n","\n","def replace_contractions(text):\n","        contractions, contractions_re = _get_contractions(contraction_dict)\n","        def replace(match):\n","            return contractions[match.group(0)]\n","        return contractions_re.sub(replace, text)\n","\n","def clean_text(txt):\n","  # 1. 소문자 변환\n","  txt = txt.lower()\n","\n","  # replace contractions\n","  txt = replace_contractions(txt)\n","\n","  # 2. HTML 태그 제거\n","  txt = re.sub(r'<.*?>', ' ', txt)\n","\n","  # 3. 구두점 제거 #!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n","  txt  = \"\".join([char for char in txt if char not in string.punctuation])\n","\n","  # 4. 숫자 제거\n","  txt = re.sub('[0-9]+', '', txt)\n","\n","  # 5. 공백 두 개 제거\n","  txt = re.sub(r\"\\s{2,}\", \" \", txt)\n","\n","  # 6. 단어 토큰화\n","  words = txt.split()\n","\n","  # 6. 불용어 제거\n","  stop_words = set(stopwords.words('english'))\n","  words = [w for w in words if not w in stop_words]\n","  #stemmer = stemmer_dict[language]\n","  #words = [stemmer.stem(word) for word in words if word not in stop_words]\n","\n","  return ' '.join(words)\n","\n","article_info['Title'] = article_info['Title'].apply(lambda txt: clean_text(txt))"],"metadata":{"id":"q8Sg-OUYZUn6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tfidf 기반 벡터화\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vect = TfidfVectorizer(min_df=0, ngram_range=(1,2))\n","content_mat = tfidf_vect.fit_transform(article_info['Title'])\n","\n","# 코사인 유사도\n","from sklearn.metrics.pairwise import cosine_similarity\n","content_similarity = cosine_similarity(content_mat, content_mat)\n","#print(content_similarity)\n","\n","# 기사 ID -> 인덱스 매핑\n","article_id_to_idx = {article: idx for idx, article in enumerate(article_info['articleID'])}\n","\n","def recommend_articles(user_id, view_log, article_similarity, article_id_to_idx, top_n=5):\n","    user_articles = view_log[view_log['userID'] == user_id]['articleID'].tolist()\n","\n","    # 조회한 기사와 유사한 기사 추출\n","    sim_scores = pd.Series(dtype=float)\n","    for article in user_articles:\n","        if article in article_id_to_idx:\n","            article_idx = article_id_to_idx[article]\n","            sim_scores = sim_scores.add(pd.Series(content_similarity[article_idx]), fill_value=0)\n","\n","    # 조회한 기사는 제외\n","    # sim_scores = sim_scores.drop([article_id_to_idx[article] for article in user_articles if article in article_id_to_idx], errors='ignore')\n","\n","    # 유사도 높은 기사 추천\n","    top_articles = sim_scores.sort_values(ascending=False).head(top_n).index\n","    top_article_ids = [list(article_id_to_idx.keys())[idx] for idx in top_articles]\n","\n","    return top_article_ids\n","\n","# 각 사용자에 대해 추천 기사 생성\n","recommendations = []\n","for user_id in sample_submission['userID'].unique():\n","    recommended_articles = recommend_articles(user_id, view_log, content_similarity, article_id_to_idx, top_n=5)\n","    for article_id in recommended_articles:\n","        recommendations.append({'userID': user_id, 'articleID': article_id})\n","\n","# 추천 결과 저장\n","submission_df = pd.DataFrame(recommendations)\n","submission_df.to_csv('제목 번역 후 클렌징.csv', index=False)"],"metadata":{"id":"IIn0nwUNmU3Q"},"execution_count":null,"outputs":[]}]}